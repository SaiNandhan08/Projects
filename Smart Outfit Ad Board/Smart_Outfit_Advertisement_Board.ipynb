{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl_Y2KvnPByY",
        "outputId": "da182324-99d5-4195-9f23-e67feac44a37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "kl_Y2KvnPByY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-9AM3oPPDeZ",
        "outputId": "5660a4ef-ed61-4dcb-e891-8efef9a3d6b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1nxIH2xjXasApCEuy6EcHF76-LsPHN_-u/fyp\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd /content/drive/MyDrive/fyp"
      ],
      "id": "e-9AM3oPPDeZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63206946"
      },
      "outputs": [],
      "source": [
        "#real time data capture:"
      ],
      "id": "63206946"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4673d03d"
      },
      "outputs": [],
      "source": [
        "#below function is to run cam on google collab"
      ],
      "id": "4673d03d"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/fyp\n",
        "\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "        js_reply: JavaScript object containing an image from the webcam\n",
        "    Returns:\n",
        "        img: OpenCV BGR image\n",
        "    \"\"\"\n",
        "    # Decode base64 image\n",
        "    image_bytes = b64decode(js_reply.split(',')[1])\n",
        "    # Convert bytes to numpy array\n",
        "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "    # Decode numpy array into OpenCV BGR image\n",
        "    img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "    return img\n",
        "\n",
        "def take_photo(filename='/content/drive/MyDrive/fyp/input_img.jpg', quality=10):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "            const div = document.createElement('div');\n",
        "            const capture = document.createElement('button');\n",
        "            capture.textContent = 'Capture';\n",
        "            div.appendChild(capture);\n",
        "\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n",
        "\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            // Resize the output to fit the video element.\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "            // Wait for Capture to be clicked.\n",
        "            await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "\n",
        "    # Get photo data\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    # Get OpenCV format image\n",
        "    img = js_to_image(data)\n",
        "\n",
        "    # Save the image\n",
        "    cv2.imwrite(filename, img)\n",
        "\n",
        "    try:\n",
        "        # Wait for a moment to ensure the image is saved\n",
        "        time.sleep(1)\n",
        "        # Show the image that was just taken\n",
        "        display(Image(filename))\n",
        "    except Exception as err:\n",
        "        print(str(err))\n",
        "\n",
        "try:\n",
        "    filename = take_photo()\n",
        "    print('Saved to {}'.format(filename))\n",
        "except Exception as err:\n",
        "    # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "    # grant the page permission to access it.\n",
        "    print(str(err))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "9cZ0HRbegQ7f",
        "outputId": "44318ffc-5289-4100-999c-ed08f42d2b89"
      },
      "id": "9cZ0HRbegQ7f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/.shortcut-targets-by-id/1nxIH2xjXasApCEuy6EcHF76-LsPHN_-u/fyp\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        async function takePhoto(quality) {\n",
              "            const div = document.createElement('div');\n",
              "            const capture = document.createElement('button');\n",
              "            capture.textContent = 'Capture';\n",
              "            div.appendChild(capture);\n",
              "\n",
              "            const video = document.createElement('video');\n",
              "            video.style.display = 'block';\n",
              "            const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n",
              "\n",
              "            document.body.appendChild(div);\n",
              "            div.appendChild(video);\n",
              "            video.srcObject = stream;\n",
              "            await video.play();\n",
              "\n",
              "            // Resize the output to fit the video element.\n",
              "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "            // Wait for Capture to be clicked.\n",
              "            await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "            const canvas = document.createElement('canvas');\n",
              "            canvas.width = video.videoWidth;\n",
              "            canvas.height = video.videoHeight;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "            stream.getVideoTracks()[0].stop();\n",
              "            div.remove();\n",
              "            return canvas.toDataURL('image/jpeg', quality);\n",
              "        }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_mimebundle_\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mmimetype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mimetype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malways_both\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmimetype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1303\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1304\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: '/content/drive/MyDrive/fyp/input_img.jpg'"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_png_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FMT_PNG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_jpeg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1303\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1304\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: '/content/drive/MyDrive/fyp/input_img.jpg'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "id": "372167fd",
        "outputId": "0bb07460-6bf8-40cf-aee7-ac68443e0d54"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to input_img.jpg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_mimebundle_\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mmimetype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mimetype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malways_both\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmimetype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1303\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1304\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'input_img.jpg'"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_png_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FMT_PNG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_jpeg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1303\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1304\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'input_img.jpg'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "\n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "id": "372167fd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49409ea4"
      },
      "outputs": [],
      "source": [
        "#below function is to run cam on jupyter and other python ide"
      ],
      "id": "49409ea4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7a1792e"
      },
      "source": [
        "#import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "#from google.colab.patches import cv2_imshow\n",
        "#cv2_imshow(img)\n",
        "\n",
        "webcam = cv2.VideoCapture(0)\n",
        "\n",
        "while(True):\n",
        "    ret, frame = webcam.read()\n",
        "    cv2_inshow(frame)\n",
        "    #cv2_imshow(\"output\",frame)\n",
        "    cv2_waitKey(1)\n",
        "\n",
        "    if cv2.waitKey(100) & 0xFF == ord('S'):\n",
        "        cv2.imwrite(\"input.jpg\", frame)\n",
        "        cv2.destroyAllWindows()\n",
        "        break\n",
        "    if cv2.waitKey(100) & 0xFF == ord('Q'):\n",
        "        cv2.destroyAllWindows()\n",
        "        break\n",
        "webcam.release()"
      ],
      "id": "c7a1792e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJKxx7bLlnse"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "id": "gJKxx7bLlnse"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "Q0BwuQO1lnsz",
        "outputId": "e3146774-c3eb-425b-9dcb-5c53e23a013c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: Failed to create the file logo.png: Read-only file system\n",
            "\r 30  4534   30  1377    0     0  12347      0 --:--:-- --:--:-- --:--:-- 12405\n",
            "curl: (23) Failure writing output to destination\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d52d15654b89>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logo.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_UNCHANGED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/patches/__init__.py\u001b[0m in \u001b[0;36mcv2_imshow\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0man\u001b[0m \u001b[0mNxM\u001b[0m \u001b[0mBGRA\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \"\"\"\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;31m# cv2 stores colors as BGR; convert to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clip'"
          ]
        }
      ],
      "source": [
        "!curl -o logo.png https://colab.research.google.com/img/colab_favicon_256px.png\n",
        "import cv2\n",
        "img = cv2.imread('logo.png', cv2.IMREAD_UNCHANGED)\n",
        "cv2_imshow(img)"
      ],
      "id": "Q0BwuQO1lnsz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6ce4316"
      },
      "outputs": [],
      "source": [
        "#Feature Extraction using YOLO:\n",
        "#checks presence of person and extracts largest boundary box which has person"
      ],
      "id": "e6ce4316"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "160e9caa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Model\n",
        "!pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"
      ],
      "id": "160e9caa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c60e92f"
      },
      "outputs": [],
      "source": [
        "\n",
        "imgpred=['input_img.jpg']\n",
        "res = model(imgpred)\n",
        "res\n"
      ],
      "id": "7c60e92f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce7b3a3e"
      },
      "outputs": [],
      "source": [
        "pred = res.pandas().xyxy[0]\n",
        "pred"
      ],
      "id": "ce7b3a3e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbbc2210"
      },
      "outputs": [],
      "source": [
        "if 0 in pred['class'].unique():\n",
        "    print('Person is detected')\n",
        "else:\n",
        "    print('No person detected')"
      ],
      "id": "bbbc2210"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0012dc84"
      },
      "outputs": [],
      "source": [
        "itr = 0\n",
        "ind = 0\n",
        "area = 0\n",
        "for index, row in pred.iterrows():\n",
        "    if str(row['class'])!=\"0\":\n",
        "        continue\n",
        "    if area < ((int(row['xmax'])-int(row['xmin']))*(int(row['ymax'])-int(row['ymin']))):\n",
        "        ind = itr\n",
        "        area = ((int(row['xmax'])-int(row['xmin']))*(int(row['ymax'])-int(row['ymin'])))\n",
        "    itr+=1\n"
      ],
      "id": "0012dc84"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "203ed3d1"
      },
      "outputs": [],
      "source": [
        "\n",
        "bbox_raq = res.xyxy[0][ind]\n",
        "bbox = []\n",
        "for bound in bbox_raq:\n",
        "    bbox.append(int(bound.item()))\n",
        "bbox"
      ],
      "id": "203ed3d1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fc2d73a"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "image = cv2.imread('input.jpg')\n",
        "img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "crp = img_rgb[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n"
      ],
      "id": "9fc2d73a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4576e8dc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image as im\n",
        "\n",
        "data = im.fromarray(crp)\n",
        "data.save('input_images/cr_img.jpg')"
      ],
      "id": "4576e8dc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93ddcedb"
      },
      "outputs": [],
      "source": [
        "#Outfit Segmentation using u2net:\n"
      ],
      "id": "93ddcedb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77cb67f8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image as im\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from data.normalize import Normalize_image\n",
        "from util.saving_utils import load_checkpoint_mgpu\n",
        "\n",
        "from networks import U2NET\n",
        "device = 'cpu'\n",
        "\n",
        "image_dir = 'input_images'\n",
        "result_dir = 'output_images'\n",
        "checkpoint_path = 'cloth_segm_u2net_latest.pth'\n",
        "\n",
        "transforms_list = []\n",
        "transforms_list += [transforms.ToTensor()]\n",
        "transforms_list += [Normalize_image(0.5, 0.5)]\n",
        "transform_rgb = transforms.Compose(transforms_list)\n",
        "\n",
        "net = U2NET(in_ch=3, out_ch=4)\n",
        "net = load_checkpoint_mgpu(net, checkpoint_path)\n",
        "net = net.to(device)\n",
        "net = net.eval()\n",
        "\n",
        "\n",
        "\n",
        "#!del -rf input_images/.ipynb_checkpoints\n",
        "image_name = sorted(os.listdir(image_dir))[0]\n",
        "\n",
        "img = im.open(os.path.join(image_dir, image_name)).convert('RGB')\n",
        "l,h= img.size\n",
        "img_size = img.size\n",
        "img = img.resize((768, 768), im.BICUBIC)\n",
        "image_tensor = transform_rgb(img)\n",
        "image_tensor = torch.unsqueeze(image_tensor, 0)\n",
        "\n",
        "output_tensor = net(image_tensor)\n",
        "output_tensor = F.log_softmax(output_tensor[0], dim=1)\n",
        "output_tensor = torch.max(output_tensor, dim=1, keepdim=True)[1]\n",
        "output_tensor = torch.squeeze(output_tensor, dim=0)\n",
        "output_tensor = torch.squeeze(output_tensor, dim=0)\n",
        "output_arr = output_tensor.cpu().numpy()\n",
        "\n",
        "output_img = im.fromarray(output_arr.astype('uint8'), mode='L')\n",
        "output_img = output_img.resize(img_size, im.BICUBIC)\n"
      ],
      "id": "77cb67f8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "816380f6"
      },
      "outputs": [],
      "source": [
        "#bottom_outfit\n",
        "\n",
        "palette = [0,0,0,0,0,0,225,225,225,0,0,0]\n",
        "\n",
        "mask= im.fromarray(output_arr.astype('uint8'),mode='L')\n",
        "mask.putpalette(palette)\n",
        "mask.save(os.path.join(result_dir, 'mask_bottom.png'))\n"
      ],
      "id": "816380f6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d96e1ac9"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "img = cv2.imread('input_images/cr_img.jpg')\n",
        "\n",
        "\n",
        "np.shape(img)\n",
        "x = cv2.imread(\"output_images/mask_bottom.png\",0)\n",
        "x=cv2.resize(x,(l,h))\n",
        "x=x.astype(\"uint8\")\n",
        "img=img.astype(\"uint8\")\n",
        "np.shape(x)\n",
        "\n",
        "res = cv2.bitwise_and(img,img,mask=x)\n",
        "image = cv2.cvtColor(res, cv2.COLOR_BGR2RGB)\n",
        "image = im.fromarray(image)\n",
        "image.save(os.path.join(result_dir,'bottom.png'))"
      ],
      "id": "d96e1ac9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6980288"
      },
      "outputs": [],
      "source": [
        "\n",
        "#top_outfit\n",
        "palette = [0,0,0,225,225,225,0,0,0,0,0,0]\n",
        "\n",
        "mask= im.fromarray(output_arr.astype('uint8'),mode='L')\n",
        "\n",
        "mask.putpalette(palette)\n",
        "mask.save(os.path.join(result_dir, 'top_mask.png'))\n"
      ],
      "id": "f6980288"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43162463"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "img = cv2.imread('input_images/cr_img.jpg')\n",
        "\n",
        "np.shape(img)\n",
        "x = cv2.imread(\"output_images/top_mask.png\",0)\n",
        "x=cv2.resize(x,(l,h))\n",
        "x=x.astype(\"uint8\")\n",
        "img=img.astype(\"uint8\")\n",
        "np.shape(x)\n",
        "\n",
        "res = cv2.bitwise_and(img,img,mask=x)\n",
        "image = cv2.cvtColor(res, cv2.COLOR_BGR2RGB)\n",
        "image = im.fromarray(image)\n",
        "image.save(os.path.join(result_dir,'top.png'))"
      ],
      "id": "43162463"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "322202f1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#fullbody_outfit\n",
        "palette = [0,0,0,0,0,0,0,0,0,225,225,225]\n",
        "\n",
        "mask= im.fromarray(output_arr.astype('uint8'),mode='L')\n",
        "\n",
        "mask.putpalette(palette)\n",
        "mask.save(os.path.join(result_dir, 'fullbody_mask.png'))\n"
      ],
      "id": "322202f1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6050a26"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "img = cv2.imread('input_images/cr_img.jpg')\n",
        "\n",
        "#img=cv2.resize(img,(768,768))\n",
        "\n",
        "np.shape(img)\n",
        "x = cv2.imread(\"output_images/fullbody_mask.png\",0)\n",
        "x=cv2.resize(x,(l,h))\n",
        "x=x.astype(\"uint8\")\n",
        "img=img.astype(\"uint8\")\n",
        "np.shape(x)\n",
        "\n",
        "res = cv2.bitwise_and(img,img,mask=x)\n",
        "image = cv2.cvtColor(res, cv2.COLOR_BGR2RGB)\n",
        "image = im.fromarray(image)\n",
        "image.save(os.path.join(result_dir,'fullbody.png'))"
      ],
      "id": "c6050a26"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "969d2daf"
      },
      "outputs": [],
      "source": [
        "#to check if fullbody image is present or not\n",
        "fb = cv2.imread(\"output_images/fullbody.png\")\n",
        "fb=cv2.resize(fb,(500,500))\n",
        "\n",
        "black = np.zeros((500,500,3),dtype=np.uint8) #make black color image\n",
        "black = 255*black\n",
        "\n",
        "isfull_body__not_present= np.array_equal(black,fb) #compare those two images if returns true then full body cloth is not present\n",
        "\n",
        "isfull_body__not_present"
      ],
      "id": "969d2daf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kejqkxURozhU"
      },
      "outputs": [],
      "source": [
        "if isfull_body__not_present==0:\n",
        "    x = cv2.imread(\"output_images/fullbody_mask.png\",0)\n",
        "    y = cv2.imread(\"output_images/top_mask.png\",0)\n",
        "    new_mask = cv2.bitwise_or(x,y)\n",
        "    image = cv2.cvtColor(new_mask, cv2.COLOR_BGR2RGB)\n",
        "    image = im.fromarray(image)\n",
        "    image.save(os.path.join(result_dir,'fullbody_mask.png'))\n",
        "    image.save(os.path.join(result_dir,'top_mask.png'))\n",
        "\n",
        "    #full_body:\n",
        "    img = cv2.imread('input_images/cr_img.jpg')\n",
        "\n",
        "    #img=cv2.resize(img,(768,768))\n",
        "\n",
        "    x = cv2.imread(\"output_images/fullbody_mask.png\",0)\n",
        "    x=cv2.resize(x,(l,h))\n",
        "    x=x.astype(\"uint8\")\n",
        "    img=img.astype(\"uint8\")\n",
        "    np.shape(x)\n",
        "\n",
        "    res = cv2.bitwise_and(img,img,mask=x)\n",
        "    image = cv2.cvtColor(res, cv2.COLOR_BGR2RGB)\n",
        "    image = im.fromarray(image)\n",
        "    image.save(os.path.join(result_dir,'fullbody.png'))\n",
        "\n",
        "    #top:\n",
        "    img = cv2.imread('input_images/cr_img.jpg')\n",
        "\n",
        "    x = cv2.imread(\"output_images/top_mask.png\",0)\n",
        "    x=cv2.resize(x,(l,h))\n",
        "    x=x.astype(\"uint8\")\n",
        "    img=img.astype(\"uint8\")\n",
        "    np.shape(x)\n",
        "\n",
        "    res = cv2.bitwise_and(img,img,mask=x)\n",
        "    image = cv2.cvtColor(res, cv2.COLOR_BGR2RGB)\n",
        "    image = im.fromarray(image)\n",
        "    image.save(os.path.join(result_dir,'top.png'))"
      ],
      "id": "kejqkxURozhU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U--QXjjdPxxF"
      },
      "outputs": [],
      "source": [
        "pip install rembg"
      ],
      "id": "U--QXjjdPxxF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cb67bfa"
      },
      "outputs": [],
      "source": [
        "\n",
        "#create white background to be applied upon top , bottom and full body outfits\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from rembg import remove\n",
        "from PIL import Image\n",
        "white = np.ones((h,l,3),dtype=np.uint8)\n",
        "white = 255*white\n",
        "\n",
        "w = cv2.cvtColor(white, cv2.COLOR_BGR2RGB)\n",
        "image = Image.fromarray(w)\n",
        "image.save(os.path.join('white_background_for_outfit.png'))"
      ],
      "id": "3cb67bfa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49cc3925"
      },
      "outputs": [],
      "source": [
        "#top image\n",
        "input = Image.open('output_images/top.png')\n",
        "mask = remove(input)\n",
        "bg = Image.open('white_background_for_outfit.png')\n",
        "outpt = Image.composite(input,bg,mask)\n",
        "outpt.save('output_images/top.png')"
      ],
      "id": "49cc3925"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "176b334e"
      },
      "outputs": [],
      "source": [
        "#bottom image\n",
        "input = Image.open('output_images/bottom.png')\n",
        "mask = remove(input)\n",
        "bg = Image.open('white_background_for_outfit.png')\n",
        "outpt = Image.composite(input,bg,mask)\n",
        "outpt.save('output_images/bottom.png')"
      ],
      "id": "176b334e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87913d3d"
      },
      "outputs": [],
      "source": [
        "#this function is to crop top half of the bottom.png to cut out the white space\n",
        "img = Image.open('output_images/bottom.png')\n",
        "\n",
        "x,y = img.size\n",
        "\n",
        "img=img.crop((0,y/2,x,y))\n",
        "img.save('output_images/bottom.png')"
      ],
      "id": "87913d3d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b17fa2f4"
      },
      "outputs": [],
      "source": [
        "#Full body\n",
        "if not isfull_body__not_present:\n",
        "\n",
        "    input = Image.open('output_images/full_body.png')\n",
        "    mask = remove(input)\n",
        "    bg = Image.open('white_background_for_outfit.png')\n",
        "    outpt = Image.composite(input,bg,mask)\n",
        "    outpt.save('output_images/full_body.png')\n",
        "else:\n",
        "    print(\"Full body outfit is not present\")"
      ],
      "id": "b17fa2f4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MCoUK4wpDq7"
      },
      "outputs": [],
      "source": [
        "#change hue of the image"
      ],
      "id": "5MCoUK4wpDq7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wis1XmWpEwr"
      },
      "outputs": [],
      "source": [
        "from torchvision.io import read_image\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.transforms as T"
      ],
      "id": "2wis1XmWpEwr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbBRjkHgpGX1"
      },
      "outputs": [],
      "source": [
        "#top\n",
        "img = read_image('output_images/top.png')\n",
        "img1 =  F.adjust_hue(img, -0.4)\n",
        "img2 =  F.adjust_hue(img, 0.2)\n",
        "\n",
        "pimg1 = T.ToPILImage()(img1)\n",
        "pimg2 = T.ToPILImage()(img2)\n",
        "\n",
        "pimg1.save('output_images/top_1.png')\n",
        "pimg2.save('output_images/top_2.png')"
      ],
      "id": "fbBRjkHgpGX1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYiS8HC8pI5-"
      },
      "outputs": [],
      "source": [
        "#bottom\n",
        "img = read_image('output_images/bottom.png')\n",
        "img1 =  F.adjust_hue(img, -0.4)\n",
        "img2 =  F.adjust_hue(img, 0.2)\n",
        "\n",
        "pimg1 = T.ToPILImage()(img1)\n",
        "pimg2 = T.ToPILImage()(img2)\n",
        "\n",
        "pimg1.save('output_images/bottom_1.png')\n",
        "pimg2.save('output_images/bottom_2.png')"
      ],
      "id": "SYiS8HC8pI5-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wgyXHBBpMbg"
      },
      "outputs": [],
      "source": [
        "#full_body\n",
        "img = read_image('output_images/fullbody.png')\n",
        "img1 =  F.adjust_hue(img, -0.4)\n",
        "img2 =  F.adjust_hue(img, 0.2)\n",
        "\n",
        "pimg1 = T.ToPILImage()(img1)\n",
        "pimg2 = T.ToPILImage()(img2)\n",
        "\n",
        "pimg1.save('output_images/fb_1.png')\n",
        "pimg2.save('output_images/fb_2.png')"
      ],
      "id": "_wgyXHBBpMbg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c518dd3"
      },
      "outputs": [],
      "source": [
        "#below code snippet takes the input of live capture module and removes the bacckground and replaces it with white background\n",
        "#the output image should be given as input image for AGCPN Virtual Try on"
      ],
      "id": "6c518dd3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6892d1d4"
      },
      "outputs": [],
      "source": [
        "img = im.open('input.jpg')\n",
        "l,h=input.size\n",
        "white = np.ones((h,l,3),dtype=np.uint8)\n",
        "white = 255*white\n",
        "\n",
        "w = cv2.cvtColor(white, cv2.COLOR_BGR2RGB)\n",
        "image = Image.fromarray(w)\n",
        "image.save(os.path.join('white_background_for_input_image.png'))"
      ],
      "id": "6892d1d4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eae29ed0"
      },
      "outputs": [],
      "source": [
        "from rembg import remove\n",
        "from PIL import Image\n",
        "input = Image.open('input.jpg')\n",
        "mask = remove(input)\n",
        "bg = Image.open('white_background_for_input_image.png')\n",
        "outpt = Image.composite(input,bg,mask)\n",
        "outpt.save('INPUT_FOR_VIRTUAL_TRY_ON.png')"
      ],
      "id": "eae29ed0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d65b08a"
      },
      "outputs": [],
      "source": [
        "#Now age and Gender classification\n",
        "!pip install DeepFace"
      ],
      "id": "6d65b08a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4657b357"
      },
      "outputs": [],
      "source": [
        "from deepface import DeepFace"
      ],
      "id": "4657b357"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67258e6a"
      },
      "outputs": [],
      "source": [
        "objs = DeepFace.analyze(img_path = \"input_images/cr_img.jpg\",\n",
        "        actions = ['age', 'gender', 'race']\n",
        ")"
      ],
      "id": "67258e6a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b2d912b"
      },
      "outputs": [],
      "source": [
        "objs"
      ],
      "id": "5b2d912b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebf4c5c7"
      },
      "outputs": [],
      "source": [
        "Age=objs[0]['age']\n",
        "Gender = objs[0]['dominant_gender']"
      ],
      "id": "ebf4c5c7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8036683"
      },
      "outputs": [],
      "source": [
        "\n",
        "if Age<=12:\n",
        "    Age_cat='child'\n",
        "elif Age<=19:\n",
        "    Age_cat='Teen'\n",
        "else:\n",
        "    Age_cat='Adult'\n",
        "\n",
        "Age_cat"
      ],
      "id": "b8036683"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "745a7ec1"
      },
      "outputs": [],
      "source": [
        "\n",
        "if Gender == 'Man':\n",
        "    Gender_cat='Male'\n",
        "else:\n",
        "    Gender_cat='Female'\n",
        "Gender_cat"
      ],
      "id": "745a7ec1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d7dcc77"
      },
      "outputs": [],
      "source": [
        "#Outfit_Classification"
      ],
      "id": "9d7dcc77"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f34745f5"
      },
      "outputs": [],
      "source": [
        "%cd outfit_classification\n",
        "!pip install pretrainedmodels"
      ],
      "id": "f34745f5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34f75b5a"
      },
      "outputs": [],
      "source": [
        "#declare the resnet-50 model\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pretrainedmodels\n",
        "\n",
        "class MultiHeadResNet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadResNet50, self).__init__()\n",
        "        self.model = pretrainedmodels.__dict__['resnet50'](pretrained=None)\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        print('Freezing intermediate layer parameters...')\n",
        "\n",
        "        # change the final layer\n",
        "        self.l0 = nn.Linear(2048, 8)\n",
        "        self.l1 = nn.Linear(2048, 57)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get the batch size only, ignore (c, h, w)\n",
        "        batch, _, _, _ = x.shape\n",
        "        x = self.model.features(x)\n",
        "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n",
        "        l0 = self.l0(x)\n",
        "        l1 = self.l1(x)\n",
        "        return l0, l1"
      ],
      "id": "34f75b5a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17c340a2"
      },
      "outputs": [],
      "source": [
        "#load the model\n",
        "import torch\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "device = torch.device('cpu')\n",
        "model = MultiHeadResNet50()\n",
        "checkpoint = torch.load('model.pth', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "id": "17c340a2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc60a01f"
      },
      "outputs": [],
      "source": [
        "\n",
        "#classification for top outfit\n",
        "image = cv2.imread('../output_images/top.png')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "# apply image transforms\n",
        "image = transform(image)\n",
        "# add batch dimension\n",
        "image = image.unsqueeze(0).to(device)\n",
        "# forward pass the image through the model\n",
        "outputs = model(image)\n",
        "# extract the three output\n",
        "output1, output2 = outputs\n",
        "# get the index positions of the highest label score\n",
        "out_label_1 = np.argmax(output1.detach().cpu())\n",
        "out_label_2 = np.argmax(output2.detach().cpu())\n",
        "num_list_art = joblib.load('./num_list_art.pkl')\n",
        "num_list_sub = joblib.load('./num_list_sub.pkl')\n",
        "# get the keys and values of each label dictionary\n",
        "art_keys = list(num_list_art.keys())\n",
        "art_values = list(num_list_art.values())\n",
        "sub_keys = list(num_list_sub.keys())\n",
        "sub_values = list(num_list_sub.values())\n",
        "final_labels = []\n",
        "# append the labels by mapping the index position to the values\n",
        "final_labels.append(art_keys[out_label_2])\n",
        "final_labels.append(sub_keys[out_label_1])\n",
        "print('article_type:'+final_labels[0])\n",
        "print('sub_cat_type:'+final_labels[1])\n",
        "\n",
        "Article_type_Top = final_labels[0]\n",
        "Category_type_Top = final_labels[0]"
      ],
      "id": "dc60a01f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fcad5d4"
      },
      "outputs": [],
      "source": [
        "\n",
        "#classification for bottom outfit\n",
        "image = cv2.imread('../output_images/bottom.png')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "# apply image transforms\n",
        "image = transform(image)\n",
        "# add batch dimension\n",
        "image = image.unsqueeze(0).to(device)\n",
        "# forward pass the image through the model\n",
        "outputs = model(image)\n",
        "# extract the three output\n",
        "output1, output2 = outputs\n",
        "# get the index positions of the highest label score\n",
        "out_label_1 = np.argmax(output1.detach().cpu())\n",
        "out_label_2 = np.argmax(output2.detach().cpu())\n",
        "num_list_art = joblib.load('./num_list_art.pkl')\n",
        "num_list_sub = joblib.load('./num_list_sub.pkl')\n",
        "# get the keys and values of each label dictionary\n",
        "art_keys = list(num_list_art.keys())\n",
        "art_values = list(num_list_art.values())\n",
        "sub_keys = list(num_list_sub.keys())\n",
        "sub_values = list(num_list_sub.values())\n",
        "final_labels = []\n",
        "# append the labels by mapping the index position to the values\n",
        "final_labels.append(art_keys[out_label_2])\n",
        "final_labels.append(sub_keys[out_label_1])\n",
        "print('article_type:'+final_labels[0])\n",
        "print('subcat:'+final_labels[1])\n",
        "Article_type_bottom = final_labels[0]\n",
        "Category_type_bottom = final_labels[0]"
      ],
      "id": "8fcad5d4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "60aae3f9"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#classification for full_body outfit\n",
        "if not isfull_body__not_present:\n",
        "\n",
        "    image = cv2.imread('../output_images/fullbody.png')\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    # apply image transforms\n",
        "    image = transform(image)\n",
        "    # add batch dimension\n",
        "    image = image.unsqueeze(0).to(device)\n",
        "    # forward pass the image through the model\n",
        "    outputs = model(image)\n",
        "    # extract the three output\n",
        "    output1, output2 = outputs\n",
        "    # get the index positions of the highest label score\n",
        "    out_label_1 = np.argmax(output1.detach().cpu())\n",
        "    out_label_2 = np.argmax(output2.detach().cpu())\n",
        "    num_list_art = joblib.load('./num_list_art.pkl')\n",
        "    num_list_sub = joblib.load('./num_list_sub.pkl')\n",
        "    # get the keys and values of each label dictionary\n",
        "    art_keys = list(num_list_art.keys())\n",
        "    art_values = list(num_list_art.values())\n",
        "    sub_keys = list(num_list_sub.keys())\n",
        "    sub_values = list(num_list_sub.values())\n",
        "    final_labels = []\n",
        "    # append the labels by mapping the index position to the values\n",
        "    final_labels.append(art_keys[out_label_2])\n",
        "    final_labels.append(sub_keys[out_label_1])\n",
        "    print('article_type:'+final_labels[0])\n",
        "    print('subcat:'+final_labels[1])\n",
        "\n",
        "\n",
        "    Article_type_fullbody = final_labels[0]\n",
        "    Category_type_fullbody = final_labels[0]\n",
        "else:\n",
        "    print(\"Full-Body outfit not present\")"
      ],
      "id": "60aae3f9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d0d2086"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Recommendation System\n",
        "%cd .."
      ],
      "id": "5d0d2086"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xE8ON7F2TGSP"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "pip install -U numpy"
      ],
      "id": "xE8ON7F2TGSP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "897fc58a"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from numpy.linalg import norm\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import cv2\n",
        "import os"
      ],
      "id": "897fc58a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7b629bb"
      },
      "outputs": [],
      "source": [
        "#Recommends top outfit and saves the images in recom-top folder"
      ],
      "id": "a7b629bb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ac39058"
      },
      "outputs": [],
      "source": [
        "# Feature_embedding = \"image_features_embedding_{}_{}_{}.pkl\".format(Gender_cat,Age_cat,Article_Type_Top/bottom/fullbody)\n",
        "# Files_name = \"img_files_{}_{}_{}.pkl\".format(Gender_cat,Age_cat,Article_Type_Top/bottom/fullbody)\n",
        "\n",
        "Feature_embedding = \"image_features_embedding_top_{}.pkl\".format(Gender_cat)\n",
        "Files_name = \"img_files_top_{}.pkl\".format(Gender_cat)"
      ],
      "id": "2ac39058"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "793cce29"
      },
      "outputs": [],
      "source": [
        "features_list = pickle.load(open(Feature_embedding, \"rb\"))\n",
        "img_files_list = pickle.load(open(Files_name, \"rb\"))\n",
        "\n",
        "\n",
        "\n",
        "model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "model.trainable = False\n",
        "i=0\n",
        "print(\"Image file names of the recommended outfit:\")\n",
        "model = Sequential([model, GlobalMaxPooling2D()])\n",
        "#0\n",
        "img = image.load_img('output_images/top.png',target_size=(224,224))\n",
        "img_array = image.img_to_array(img)\n",
        "expand_img = np.expand_dims(img_array,axis=0)\n",
        "preprocessed_img = preprocess_input(expand_img)\n",
        "result_to_resnet = model.predict(preprocessed_img)\n",
        "flatten_result = result_to_resnet.flatten()\n",
        "# normalizing\n",
        "result_normlized = flatten_result / norm(flatten_result)\n",
        "\n",
        "\n",
        "neighbors = NearestNeighbors(n_neighbors = 6, algorithm='brute', metric='euclidean')\n",
        "neighbors.fit(features_list)\n",
        "\n",
        "distence, indices = neighbors.kneighbors([result_normlized])\n",
        "\n",
        "for file in indices[0][0:1]:\n",
        "    print(\"{}.jpg\".format(img_files_list[file]))\n",
        "    tmp_img = cv2.imread(os.path.join(\"Inventory-data\",str(img_files_list[file])+\".jpg\"),cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    cv2.imwrite(os.path.join(\"recom-top\", str(i)+'reco.png'),tmp_img)\n",
        "    i+=1\n",
        "\n",
        "#1\n",
        "img = image.load_img('output_images/top_1.png',target_size=(224,224))\n",
        "img_array = image.img_to_array(img)\n",
        "expand_img = np.expand_dims(img_array,axis=0)\n",
        "preprocessed_img = preprocess_input(expand_img)\n",
        "result_to_resnet = model.predict(preprocessed_img)\n",
        "flatten_result = result_to_resnet.flatten()\n",
        "# normalizing\n",
        "result_normlized = flatten_result / norm(flatten_result)\n",
        "\n",
        "\n",
        "neighbors = NearestNeighbors(n_neighbors = 6, algorithm='brute', metric='euclidean')\n",
        "neighbors.fit(features_list)\n",
        "\n",
        "distence, indices = neighbors.kneighbors([result_normlized])\n",
        "\n",
        "for file in indices[0][0:1]:\n",
        "    print(\"{}.jpg\".format(img_files_list[file]))\n",
        "    tmp_img = cv2.imread(os.path.join(\"Inventory-data\",str(img_files_list[file])+\".jpg\"),cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    cv2.imwrite(os.path.join(\"recom-top\", str(i)+'reco.png'),tmp_img)\n",
        "    i+=1\n",
        "#2\n",
        "img = image.load_img('output_images/top_2.png',target_size=(224,224))\n",
        "img_array = image.img_to_array(img)\n",
        "expand_img = np.expand_dims(img_array,axis=0)\n",
        "preprocessed_img = preprocess_input(expand_img)\n",
        "result_to_resnet = model.predict(preprocessed_img)\n",
        "flatten_result = result_to_resnet.flatten()\n",
        "# normalizing\n",
        "result_normlized = flatten_result / norm(flatten_result)\n",
        "\n",
        "\n",
        "neighbors = NearestNeighbors(n_neighbors = 6, algorithm='brute', metric='euclidean')\n",
        "neighbors.fit(features_list)\n",
        "\n",
        "distence, indices = neighbors.kneighbors([result_normlized])\n",
        "\n",
        "for file in indices[0][0:1]:\n",
        "    print(\"{}.jpg\".format(img_files_list[file]))\n",
        "    tmp_img = cv2.imread(os.path.join(\"Inventory-data\",str(img_files_list[file])+\".jpg\"),cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    cv2.imwrite(os.path.join(\"recom-top\", str(i)+'reco.png'),tmp_img)\n",
        "    i+=1"
      ],
      "id": "793cce29"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44db1a01"
      },
      "outputs": [],
      "source": [
        "#Recommends bottom outfits and saves the images in recom-bottom folder\n",
        "\n",
        "Feature_embedding = \"image_features_embedding_bottom_{}.pkl\".format(Gender_cat)\n",
        "Files_name = \"img_files_bottom_{}.pkl\".format(Gender_cat)"
      ],
      "id": "44db1a01"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7999d81e"
      },
      "outputs": [],
      "source": [
        "features_list = pickle.load(open(Feature_embedding, \"rb\"))\n",
        "img_files_list = pickle.load(open(Files_name, \"rb\"))\n",
        "\n",
        "model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "model.trainable = False\n",
        "\n",
        "model = Sequential([model, GlobalMaxPooling2D()])\n",
        "print(\"Image file names of the recommended outfit:\")\n",
        "\n",
        "i=0\n",
        "#0\n",
        "img = image.load_img('output_images/bottom.png',target_size=(224,224))\n",
        "img_array = image.img_to_array(img)\n",
        "expand_img = np.expand_dims(img_array,axis=0)\n",
        "preprocessed_img = preprocess_input(expand_img)\n",
        "result_to_resnet = model.predict(preprocessed_img)\n",
        "flatten_result = result_to_resnet.flatten()\n",
        "# normalizing\n",
        "result_normlized = flatten_result / norm(flatten_result)\n",
        "\n",
        "\n",
        "neighbors = NearestNeighbors(n_neighbors = 6, algorithm='brute', metric='euclidean')\n",
        "neighbors.fit(features_list)\n",
        "\n",
        "distence, indices = neighbors.kneighbors([result_normlized])\n",
        "\n",
        "for file in indices[0][0:1]:\n",
        "    print(\"{}.jpg\".format(img_files_list[file]))\n",
        "    tmp_img = cv2.imread(os.path.join(\"Inventory-data\",str(img_files_list[file])+\".jpg\"),cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    cv2.imwrite(os.path.join(\"recom-bottom\", str(i)+'reco.png'),tmp_img)\n",
        "    i+=1\n",
        "\n",
        "#1\n",
        "img = image.load_img('output_images/bottom_1.png',target_size=(224,224))\n",
        "img_array = image.img_to_array(img)\n",
        "expand_img = np.expand_dims(img_array,axis=0)\n",
        "preprocessed_img = preprocess_input(expand_img)\n",
        "result_to_resnet = model.predict(preprocessed_img)\n",
        "flatten_result = result_to_resnet.flatten()\n",
        "# normalizing\n",
        "result_normlized = flatten_result / norm(flatten_result)\n",
        "\n",
        "\n",
        "neighbors = NearestNeighbors(n_neighbors = 6, algorithm='brute', metric='euclidean')\n",
        "neighbors.fit(features_list)\n",
        "\n",
        "distence, indices = neighbors.kneighbors([result_normlized])\n",
        "\n",
        "for file in indices[0][0:1]:\n",
        "    print(\"{}.jpg\".format(img_files_list[file]))\n",
        "    tmp_img = cv2.imread(os.path.join(\"Inventory-data\",str(img_files_list[file])+\".jpg\"),cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    cv2.imwrite(os.path.join(\"recom-bottom\", str(i)+'reco.png'),tmp_img)\n",
        "    i+=1\n",
        "#2\n",
        "img = image.load_img('output_images/bottom_2.png',target_size=(224,224))\n",
        "img_array = image.img_to_array(img)\n",
        "expand_img = np.expand_dims(img_array,axis=0)\n",
        "preprocessed_img = preprocess_input(expand_img)\n",
        "result_to_resnet = model.predict(preprocessed_img)\n",
        "flatten_result = result_to_resnet.flatten()\n",
        "# normalizing\n",
        "result_normlized = flatten_result / norm(flatten_result)\n",
        "\n",
        "\n",
        "neighbors = NearestNeighbors(n_neighbors = 6, algorithm='brute', metric='euclidean')\n",
        "neighbors.fit(features_list)\n",
        "\n",
        "distence, indices = neighbors.kneighbors([result_normlized])\n",
        "\n",
        "for file in indices[0][0:1]:\n",
        "    print(\"{}.jpg\".format(img_files_list[file]))\n",
        "    tmp_img = cv2.imread(os.path.join(\"Inventory-data\",str(img_files_list[file])+\".jpg\"),cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    cv2.imwrite(os.path.join(\"recom-bottom\", str(i)+'reco.png'),tmp_img)\n",
        "    i+=1"
      ],
      "id": "7999d81e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "699de2e8"
      },
      "outputs": [],
      "source": [
        "#Recommends full-body outfits and saves the images in recom-fullbody folder"
      ],
      "id": "699de2e8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "662221ba"
      },
      "outputs": [],
      "source": [
        "if not isfull_body__not_present:\n",
        "    features_list = pickle.load(open(Feature_embedding, \"rb\"))\n",
        "    img_files_list = pickle.load(open(Files_name, \"rb\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "    model.trainable = False\n",
        "\n",
        "    model = Sequential([model, GlobalMaxPooling2D()])\n",
        "\n",
        "    img = image.load_img('output_images/fullbody.png',target_size=(224,224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    expand_img = np.expand_dims(img_array,axis=0)\n",
        "    preprocessed_img = preprocess_input(expand_img)\n",
        "    result_to_resnet = model.predict(preprocessed_img)\n",
        "    flatten_result = result_to_resnet.flatten()\n",
        "    # normalizing\n",
        "    result_normlized = flatten_result / norm(flatten_result)\n",
        "\n",
        "\n",
        "    neighbors = NearestNeighbors(n_neighbors = 6, algorithm='brute', metric='euclidean')\n",
        "    neighbors.fit(features_list)\n",
        "\n",
        "    distence, indices = neighbors.kneighbors([result_normlized])\n",
        "\n",
        "    print(indices)\n",
        "    i=0\n",
        "    for file in indices[0][0:3]:\n",
        "        print(\"{}.jpg\".format(img_files_list[file]))\n",
        "        tmp_img = cv2.imread(os.path.join(\"Inventory-data\",str(img_files_list[file])+\".jpg\"),cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        cv2.imwrite(os.path.join(\"recom-fullbody\", str(i)+'reco.png'),tmp_img)\n",
        "else:\n",
        "    print(\"Full-Body image not present, so recommendations cannot be made\")"
      ],
      "id": "662221ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk3cxm4FbaLb"
      },
      "source": [
        "THE TOP RECOMMENDED OUTFIT IS USED FOR VIRTUAL TRY-ON\n"
      ],
      "id": "gk3cxm4FbaLb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9ce6bbc"
      },
      "outputs": [],
      "source": [
        "%cd ACGPN/"
      ],
      "id": "b9ce6bbc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsw4URjqWkpq"
      },
      "outputs": [],
      "source": [
        "!pip install ninja"
      ],
      "id": "bsw4URjqWkpq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Jm9cbKyWm1N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import IPython\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from predict_pose import generate_pose_keypoints"
      ],
      "id": "3Jm9cbKyWm1N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9t5_BDyWpxN"
      },
      "outputs": [],
      "source": [
        "!rm -rf Data_preprocessing/test_color/*\n",
        "!rm -rf Data_preprocessing/test_colormask/*\n",
        "!rm -rf Data_preprocessing/test_edge/*\n",
        "!rm -rf Data_preprocessing/test_img/*\n",
        "!rm -rf Data_preprocessing/test_label/*\n",
        "!rm -rf Data_preprocessing/test_mask/*\n",
        "!rm -rf Data_preprocessing/test_pose/*\n",
        "!rm -rf inputs/cloth/*\n",
        "!rm -rf inputs/img/*\n",
        "!rm -rf results/*"
      ],
      "id": "o9t5_BDyWpxN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJh0eL-aWypZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "%cd U-2-Net\n",
        "import u2net_load\n",
        "import u2net_run\n",
        "u2net = u2net_load.model(model_name = 'u2netp')\n",
        "%cd .."
      ],
      "id": "gJh0eL-aWypZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPOGlgMGW1Ic"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "u2net_run.infer(u2net, '', 'Data_preprocessing/test_edge')"
      ],
      "id": "uPOGlgMGW1Ic"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQSYY5BVW4Jr"
      },
      "outputs": [],
      "source": [
        "cloth_name = '000001_1.png'\n",
        "cloth_path = '/content/drive/MyDrive/fyp/recom-top/2reco.png'\n",
        "cloth = Image.open(cloth_path)\n",
        "cloth = cloth.resize((192, 256), Image.BICUBIC).convert('RGB')\n",
        "cloth.save(os.path.join('Data_preprocessing/test_color', cloth_name))\n",
        "\n",
        "u2net_run.infer(u2net, 'Data_preprocessing/test_color', 'Data_preprocessing/test_edge')"
      ],
      "id": "aQSYY5BVW4Jr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7OR2eRdXPAV"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "img_name = '000001_0.png'\n",
        "img_path ='/content/drive/MyDrive/fyp/INPUT_FOR_VIRTUAL_TRY_ON.png'\n",
        "img = Image.open(img_path)\n",
        "img = img.resize((192,256), Image.BICUBIC)\n",
        "\n",
        "img_path = os.path.join('Data_preprocessing/test_img', img_name)\n",
        "img.save(img_path)\n",
        "resize_time = time.time()\n",
        "print('Resized image in {}s'.format(resize_time-start_time))\n",
        "\n",
        "!python3 Self-Correction-Human-Parsing-for-ACGPN/simple_extractor.py --dataset 'lip' --model-restore 'lip_final.pth' --input-dir 'Data_preprocessing/test_img' --output-dir 'Data_preprocessing/test_label'\n",
        "parse_time = time.time()\n",
        "print('Parsing generated in {}s'.format(parse_time-resize_time))\n"
      ],
      "id": "U7OR2eRdXPAV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZaMlEuAXYTj"
      },
      "outputs": [],
      "source": [
        "\n",
        "pose_path = os.path.join('Data_preprocessing/test_pose', img_name.replace('.png', '_keypoints.json'))\n",
        "generate_pose_keypoints(img_path, pose_path)\n",
        "pose_time = time.time()\n",
        "print('Pose map generated in {}s'.format(pose_time-parse_time))"
      ],
      "id": "hZaMlEuAXYTj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Igz6YAwGXmcP"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open('Data_preprocessing/test_pairs.txt','w') as f:\n",
        "    f.write('000001_0.png 000001_1.png')"
      ],
      "id": "Igz6YAwGXmcP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmncEczbXoYs"
      },
      "outputs": [],
      "source": [
        "!python test.py"
      ],
      "id": "EmncEczbXoYs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwoxyijwXrP3"
      },
      "outputs": [],
      "source": [
        "output_grid = np.concatenate([np.array(Image.open('Data_preprocessing/test_img/000001_0.png')),\n",
        "                np.array(Image.open('Data_preprocessing/test_color/000001_1.png')),\n",
        "                np.array(Image.open('results/test/try-on/000001_0.png'))], axis=1)\n",
        "image_grid = Image.fromarray(output_grid)\n",
        "image_grid"
      ],
      "id": "iwoxyijwXrP3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkE3q03MYP78"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ],
      "id": "zkE3q03MYP78"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF8GtVU82N3d"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/megvii-research/NAFNet\n",
        "%cd NAFNet"
      ],
      "id": "gF8GtVU82N3d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_Daxxn0qfco"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!python3 setup.py develop --no_cuda_ext"
      ],
      "id": "C_Daxxn0qfco"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dNCZcSYqhNe"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=14Fht1QQJ2gMlk4N1ERCRuElg8JfjrWWR', \"./experiments/pretrained_models/\", quiet=False)"
      ],
      "id": "9dNCZcSYqhNe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7BlfF_bql0i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from basicsr.models import create_model\n",
        "from basicsr.utils import img2tensor as _img2tensor, tensor2img, imwrite\n",
        "from basicsr.utils.options import parse\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def imread(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  return img\n",
        "def img2tensor(img, bgr2rgb=False, float32=True):\n",
        "    img = img.astype(np.float32) / 255.\n",
        "    return _img2tensor(img, bgr2rgb=bgr2rgb, float32=float32)\n",
        "\n",
        "def display(img1, img2):\n",
        "  fig = plt.figure(figsize=(25, 10))\n",
        "  ax1 = fig.add_subplot(1, 2, 1)\n",
        "  plt.title('Input image', fontsize=16)\n",
        "  ax1.axis('off')\n",
        "  ax2 = fig.add_subplot(1, 2, 2)\n",
        "  plt.title('NAFNet output', fontsize=16)\n",
        "  ax2.axis('off')\n",
        "  ax1.imshow(img1)\n",
        "  ax2.imshow(img2)\n",
        "\n",
        "def single_image_inference(model, img, save_path):\n",
        "      model.feed_data(data={'lq': img.unsqueeze(dim=0)})\n",
        "\n",
        "      if model.opt['val'].get('grids', False):\n",
        "          model.grids()\n",
        "\n",
        "      model.test()\n",
        "\n",
        "      if model.opt['val'].get('grids', False):\n",
        "          model.grids_inverse()\n",
        "\n",
        "      visuals = model.get_current_visuals()\n",
        "      sr_img = tensor2img([visuals['result']])\n",
        "      imwrite(sr_img, save_path)\n"
      ],
      "id": "U7BlfF_bql0i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En6iahopqotX"
      },
      "outputs": [],
      "source": [
        "opt_path = 'options/test/SIDD/NAFNet-width64.yml'\n",
        "opt = parse(opt_path, is_train=False)\n",
        "opt['dist'] = False\n",
        "NAFNet = create_model(opt)"
      ],
      "id": "En6iahopqotX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh4PrSLyqqrL"
      },
      "outputs": [],
      "source": [
        "input_path = '/content/drive/MyDrive/fyp/ACGPN/results/test/try-on/000001_0.png'\n",
        "output_path = 'demo_output/noisy-demo-0.png'\n",
        "\n",
        "img_input = imread(input_path)\n",
        "inp = img2tensor(img_input)\n",
        "single_image_inference(NAFNet, inp, output_path)\n",
        "img_output = imread(output_path)\n",
        "display(img_input, img_output)"
      ],
      "id": "Gh4PrSLyqqrL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMEavTc4q9St"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=14D4V4raNYIOhETfcuuLI3bGLB-OYIv6X', \"./experiments/pretrained_models/\", quiet=False)"
      ],
      "id": "dMEavTc4q9St"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcgzvB6orSdk"
      },
      "outputs": [],
      "source": [
        "opt_path = 'options/test/REDS/NAFNet-width64.yml'\n",
        "opt = parse(opt_path, is_train=False)\n",
        "opt['dist'] = False\n",
        "NAFNet = create_model(opt)"
      ],
      "id": "OcgzvB6orSdk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taT0d_9trbRi"
      },
      "outputs": [],
      "source": [
        "input_path = '/content/drive/MyDrive/fyp/ACGPN/results/test/try-on/000001_0.png'\n",
        "output_path = 'demo_output/noisy-demo-0.png'\n",
        "\n",
        "img_input = imread(input_path)\n",
        "inp = img2tensor(img_input)\n",
        "single_image_inference(NAFNet, inp, output_path)\n",
        "img_output = imread(output_path)\n",
        "display(img_input, img_output)"
      ],
      "id": "taT0d_9trbRi"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MCMP-_tGtzB6"
      },
      "id": "MCMP-_tGtzB6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac3bmmcirfxk"
      },
      "outputs": [],
      "source": [],
      "id": "Ac3bmmcirfxk"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fyp",
      "language": "python",
      "name": "fyp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}